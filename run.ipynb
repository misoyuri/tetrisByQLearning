{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rkdtj\\tetris\\logs.py:8: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dqn_agent import DQNAgent\n",
    "from tetris import Tetris\n",
    "from datetime import datetime\n",
    "from statistics import mean, median\n",
    "import random\n",
    "from logs import CustomTensorBoard\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                               | 21/2000 [00:13<22:54,  1.44it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-eaa551da0c52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-eaa551da0c52>\u001b[0m in \u001b[0;36mdqn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m#dqn_agent.play()에 args를 넘기고 reward와 경기 종료 유무 받아 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             reward, done = env.play(best_action[0], best_action[1], render=render,\n\u001b[1;32m---> 57\u001b[1;33m                                     render_delay=render_delay)\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m#train을 위해 memory buffer에 학습자료 넣기.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tetris\\tetris.py\u001b[0m in \u001b[0;36mplay\u001b[1;34m(self, x, rotation, render, render_delay)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_collision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_rotated_piece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mrender_delay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                     \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrender_delay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\tetris\\tetris.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# RRG에서 BGR 로 전환한다.(used by cv2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTetris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBOARD_WIDTH\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTetris\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBOARD_HEIGHT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box)\u001b[0m\n\u001b[0;32m   1890\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1892\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1894\u001b[0m     def rotate(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dqn():\n",
    "    #model 학습을 위한 args를 만들기\n",
    "    env = Tetris()\n",
    "    episodes = 2000\n",
    "    max_steps = None\n",
    "    epsilon_stop_episode = 1500\n",
    "    mem_size = 20000\n",
    "    discount = 0.95\n",
    "    batch_size = 128\n",
    "    epochs = 15\n",
    "    render_every = 1\n",
    "    log_every = 50\n",
    "    replay_start_size = 2000\n",
    "    train_every = 1\n",
    "    n_neurons = [32, 32]\n",
    "    render_delay = None\n",
    "    activations = ['relu', 'relu', 'linear']\n",
    "\n",
    "    agent = DQNAgent(env.get_state_size(),\n",
    "                     n_neurons=n_neurons, activations=activations,\n",
    "                     epsilon_stop_episode=epsilon_stop_episode, mem_size=mem_size,\n",
    "                     discount=discount, replay_start_size=replay_start_size)\n",
    "\n",
    "    # log data를 저장하기위해 directory를 만들고 log data 저장을 위해 tensorboard를 이용\n",
    "    log_dir = f'logs/tetris-nn={str(n_neurons)}-mem={mem_size}-bs={batch_size}-e={epochs}-{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "    log = CustomTensorBoard(log_dir=log_dir)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # 설정해둔 episode만큼 게임을 진행\n",
    "    # 여기서 'tqdm'은 아래에 진행 상황을 보여주는 bar를 위해 이용됨.\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        # 경기 렌더링을 몇번의 episode마다 진행할것인지 결정\n",
    "        if render_every and episode % render_every == 0:\n",
    "            render = True\n",
    "        else:\n",
    "            render = False\n",
    "\n",
    "        # done(false: not game over, true: game over)\n",
    "        # 경기가 끝날 때까지 진행한다. (done!=true)\n",
    "        while not done and (not max_steps or steps < max_steps):\n",
    "            next_states = env.get_next_states()\n",
    "            best_state = agent.best_state(next_states.values())\n",
    "            \n",
    "            best_action = None\n",
    "            for action, state in next_states.items():\n",
    "                if state == best_state:\n",
    "                    best_action = action\n",
    "                    break\n",
    "            \n",
    "            #dqn_agent.play()에 args를 넘기고 reward와 경기 종료 유무 받아 저장\n",
    "            reward, done = env.play(best_action[0], best_action[1], render=render,\n",
    "                                    render_delay=render_delay)\n",
    "            \n",
    "            #train을 위해 memory buffer에 학습자료 넣기.\n",
    "            agent.add_to_memory(current_state, next_states[best_action], reward, done)\n",
    "            current_state = next_states[best_action]\n",
    "            steps += 1\n",
    "\n",
    "        scores.append(env.get_game_score())\n",
    "\n",
    "        # train_every로 정해둔 만큼마다 train을 시도\n",
    "        if episode % train_every == 0:\n",
    "            agent.train(batch_size=batch_size, epochs=epochs)\n",
    "            \n",
    "\n",
    "        # log_every로 정해둔 만큼마다 설정해둔 dir에 log를 생성한다.\n",
    "        # log의 들어가는 data는 log_every만큼의 episode에 대해서 평균 점수, 최저 점수, 최고 점수를 기록한다.\n",
    "        if log_every and episode and episode % log_every == 0:\n",
    "            avg_score = mean(scores[-log_every:])\n",
    "            min_score = min(scores[-log_every:])\n",
    "            max_score = max(scores[-log_every:])\n",
    "\n",
    "            log.log(episode, avg_score=avg_score, min_score=min_score,\n",
    "                    max_score=max_score)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
