{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    '''\n",
    "    Args:\n",
    "        state_size (int): 입력값의 크기\n",
    "        mem_size (int): train에 필요한 buffer 크기\n",
    "        discount (float): 0~1사이 값으로 현재 reward보다 미래의 reward가 얼마나 중요한지 결정하는 변수\n",
    "        epsilon (float): 탐험을 위한 기준 값\n",
    "        epsilon_min (float): epsilon의 최소 값\n",
    "        epsilon_stop_episode (int): episode 몇에서 감소를 멈출지 결정 하는 변수\n",
    "        n_neurons (list(int)): inner layer의 뉴런 수가 담긴 list\n",
    "        activations (list): inner layer에 따른 activation function list\n",
    "        loss (obj): Loss function\n",
    "        optimizer (obj): 학습률을 줄여나가고 속도를 계산하여 학습의 갱신강도를 적응적으로 조정해나가는 방법\n",
    "        replay_start_size: 학습에 필요한 최소 값\n",
    "    '''\n",
    "\n",
    "    def __init__(self, state_size, mem_size=10000, discount=0.95,\n",
    "                 epsilon=1, epsilon_min=0, epsilon_stop_episode=500,\n",
    "                 n_neurons=[32, 32], activations=['relu', 'relu', 'linear'],\n",
    "                 loss='mse', optimizer='adam', replay_start_size=None):\n",
    "\n",
    "        assert len(activations) == len(n_neurons) + 1\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.memory = deque(maxlen=mem_size)\n",
    "        self.discount = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / (epsilon_stop_episode)\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activations = activations\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        if not replay_start_size:\n",
    "            replay_start_size = mem_size / 2\n",
    "        self.replay_start_size = replay_start_size\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # keras 인공신경망 모델 생성\n",
    "        model = Sequential()\n",
    "        # n_nourons[0]만큼 node를 생성하고, input_dim은 input data로 부터 몇개의 값이 들어올지 정해주는 것.\n",
    "        model.add(Dense(self.n_neurons[0], input_dim=self.state_size, activation=self.activations[0]))\n",
    "\n",
    "        # hidden layer 형성\n",
    "        for i in range(1, len(self.n_neurons)):\n",
    "            model.add(Dense(self.n_neurons[i], activation=self.activations[i]))\n",
    "        # Output layer 형성\n",
    "        model.add(Dense(1, activation=self.activations[-1]))\n",
    "\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def add_to_memory(self, current_state, next_state, reward, done):\n",
    "        '''Adds a play to the replay memory buffer'''\n",
    "        # 'done'은 경기가 진행중인지 끝났는지에 대한 bool.\n",
    "        self.memory.append((current_state, next_state, reward, done))\n",
    "\n",
    "    def random_value(self):\n",
    "        # random한 값을 return\n",
    "        return random.random()\n",
    "\n",
    "    def predict_value(self, state):\n",
    "        # state에 대한 예측 값을 return\n",
    "        return self.model.predict(state)[0]\n",
    "\n",
    "    def act(self, state):\n",
    "        # 더 좋은 방법을 모험적으로 찾기 위해서 기존에 정해 놓은 epsilon보다 random value가 작으면 random_value를 return\n",
    "        # 그렇지 않다면 predict_value를 return\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        if random.random() <= self.epsilon:\n",
    "            return self.random_value()\n",
    "        else:\n",
    "            return self.predict_value(state)\n",
    "\n",
    "    def best_state(self, states):\n",
    "        '''Returns the best state for a given collection of states'''\n",
    "        # 주어진 states에서 최고의 state를 찾아서 리턴한다.\n",
    "        # 여기서 또한 모험적으로 더 좋은 방법을 찾기위해 epsilon보다 random vlaue가 작으면 random하게 골라서 return\n",
    "        max_value = None\n",
    "        best_state = None\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.choice(list(states))\n",
    "\n",
    "        else:\n",
    "            for state in states:\n",
    "                value = self.predict_value(np.reshape(state, [1, self.state_size]))\n",
    "                if not max_value or value > max_value:\n",
    "                    max_value = value\n",
    "                    best_state = state\n",
    "\n",
    "        return best_state\n",
    "\n",
    "    def train(self, batch_size=32, epochs=3):\n",
    "        # model을 학습\n",
    "        n = len(self.memory)\n",
    "\n",
    "        # memory buffer에 들어있는 값의 수가 batch_size와 replay_start_size보다 클 때 학습을 진행.\n",
    "        if n >= self.replay_start_size and n >= batch_size:\n",
    "\n",
    "            batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "            # batch에서 array를 만들고 predict()를 이용해 next_states의 출력값을 next_qs에 담는다.\n",
    "            next_states = np.array([x[1] for x in batch])\n",
    "            next_qs = [x[0] for x in self.model.predict(next_states)]\n",
    "\n",
    "            x = []\n",
    "            y = []\n",
    "\n",
    "            # batch에서 model 학습을 위해 xy structure를 만든다.\n",
    "            for i, (state, _, reward, done) in enumerate(batch):\n",
    "                if not done:\n",
    "                    # Partial Q formula\n",
    "                    new_q = reward + self.discount * next_qs[i]\n",
    "                else:\n",
    "                    new_q = reward\n",
    "\n",
    "                x.append(state)\n",
    "                y.append(new_q)\n",
    "\n",
    "            # 주어진 값에 따라서 model을 학습시킨다.\n",
    "            self.model.fit(np.array(x), np.array(y), batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "\n",
    "            # 탐색을 위한 epsilon값을 update한다.\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon -= self.epsilon_decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
